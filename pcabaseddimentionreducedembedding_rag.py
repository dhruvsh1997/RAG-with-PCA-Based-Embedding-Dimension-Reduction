# -*- coding: utf-8 -*-
"""PCABasedDimentionReducedEmbedding-RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pjlvwRXNeo-Pp_uCPhT6A9I1ohHKqFvP

## 2. `ChatPromptTemplate`

### Description:
- `ChatPromptTemplate` is a high-level template designed for **chat-based models**.
- Instead of formatting one single string, it organizes a sequence of messages (system, human, AI, etc.).
- It creates `ChatMessages` which are then sent to chat-based models like GPT-3.5, GPT-4, Claude, etc.

### Key Use-Cases:
- Required for multi-turn conversations or contextual instructions.
- Common in advanced RAG pipelines using `ChatOpenAI` or `ChatAnthropic`.
"""

# ===================== INSTALL DEPENDENCIES =====================
!pip install -q langchain sentence-transformers faiss-cpu pypdf groq langchain-community langchain-groq scikit-learn

# ===================== IMPORTS =====================
import os
import torch
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.schema.messages import SystemMessage, AIMessage
from langchain_core.prompts import HumanMessagePromptTemplate
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains import RetrievalQA
from langchain_groq import ChatGroq
import numpy as np
from sklearn.decomposition import PCA
from sentence_transformers.cross_encoder import CrossEncoder

import pandas as pd
from IPython.display import display, Markdown

# ===================== LOAD & SPLIT PDF =====================
loader = PyPDFLoader("/content/solid-python.pdf")
documents = loader.load_and_split()

splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
docs = splitter.split_documents(documents)
print(f"Total Chunks Created: {len(docs)}")

# Store reduced vectors manually in FAISS
from langchain.vectorstores.faiss import FAISS
from langchain.embeddings.base import Embeddings
from sentence_transformers import SentenceTransformer
from typing import List
# ===================== EMBEDDINGS + VECTORSTORE =====================
st_model = SentenceTransformer("all-MiniLM-L6-v2")
# Step 1: Extract text chunks
texts = [doc.page_content for doc in docs]
# Step 2: Generate 384D embeddings
original_embeddings = st_model.encode(texts, convert_to_numpy=True)

original_embeddings.shape

# ===================== PCA DIM REDUCTION =====================
# Perform PCA to reduce 384D â†’ 22D (since we have 22 samples)
pca = PCA(n_components=18)
# Extract vectors from FAISS object - This line is incorrect, original_embeddings is already a numpy array
reduced_embeddings = pca.fit_transform(original_embeddings)
# Custom embedding wrapper to use PCA-reduced vectors
class PCAEmbeddings(Embeddings):
    def __init__(self, model, pca):
        self.model = model
        self.pca = pca

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        vectors = self.model.encode(texts, convert_to_numpy=True)
        return self.pca.transform(vectors).tolist()

    def embed_query(self, text: str) -> List[float]:
        vector = self.model.encode([text], convert_to_numpy=True)
        return self.pca.transform(vector)[0].tolist()
# Wrap with PCA reducer
embedding_wrapper = PCAEmbeddings(st_model, pca)
vectorstore = FAISS.from_documents(docs, embedding_wrapper)
retriever = vectorstore.as_retriever()

reduced_embeddings.shape

# ===================== DEFINE LLM =====================
from google.colab import userdata
llm = ChatGroq(
    model_name="llama-3.3-70b-versatile",
    api_key=userdata.get('GROQ_API_KEY')  # Replace with your Groq API key
)

# ===================== DEFINE PROMPT ===================
chat_prompt = ChatPromptTemplate.from_messages([
    SystemMessage(content="You are a helpful assistant that answers based only on the given context."),
    HumanMessagePromptTemplate.from_template("Given the following context:\n\n{context}\n\nAnswer the question:\n\n{question}")
])

# ===================== RERANKER =====================
reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L6-v2")

question = "What is the main objective of the document?"
# Step 1: Retrieve top chunks using MMR
retrieved_docs = retriever.get_relevant_documents(question)

# Show pre-reranked chunks
print("\nðŸ”¹ Top K Retrieved Chunks (Before Reranking):")
for i, doc in enumerate(retrieved_docs):
    print(f"\n--- Chunk {i+1} ---")
    print(f"Page: {doc.metadata.get('page', 'Unknown')}")
    print(f"Content:\n{doc.page_content[:300]}...")

# Step 2: Rerank the retrieved chunks using cross-encoder
pairs = [[question, doc.page_content] for doc in retrieved_docs]
scores = reranker.predict(pairs)
scored_docs = list(zip(retrieved_docs, scores))
sorted_docs = sorted(scored_docs, key=lambda x: x[1], reverse=True)

# Show reranked chunks
print("\nðŸ”¸ Reranked Chunks (CrossEncoder):")
for i, (doc, score) in enumerate(sorted_docs):
    print(f"\n--- Reranked Chunk {i+1} ---")
    print(f"Page: {doc.metadata.get('page', 'Unknown')}")
    print(f"Score: {score:.4f}")
    print(f"Content:\n{doc.page_content[:300]}...")

# Answer using original MMR top chunks
context_before = "\n\n".join([doc.page_content for doc in retrieved_docs[:3]])
messages_before = chat_prompt.format_messages(context=context_before, question=question)
answer_before = llm.invoke(messages_before)

display(Markdown("### Final Answer (Before Reranking):"))
display(Markdown(answer_before.content))

# Answer using reranked top chunks
top_reranked_docs = [doc for doc, _ in sorted_docs[:3]]
context_after = "\n\n".join([doc.page_content for doc in top_reranked_docs])
messages_after = chat_prompt.format_messages(context=context_after, question=question)
answer_after = llm.invoke(messages_after)

display(Markdown("### Final Answer (After Reranking):"))
display(Markdown(answer_after.content))

